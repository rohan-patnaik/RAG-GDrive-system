# backend/rag_system/core/vector_store.py
import logging
from typing import List, Optional, Dict, Any, Tuple

import chromadb
from chromadb.api.models.Collection import Collection as ChromaCollection
from chromadb.utils import embedding_functions

from rag_system.models.schemas import DocumentChunk, DocumentMetadata, RetrievedChunk
from rag_system.utils.exceptions import EmbeddingError, VectorStoreError
from rag_system.config.settings import AppSettings, get_settings
from rag_system.core.embeddings import EmbeddingService # For embedding dimension

logger = logging.getLogger(__name__)


class VectorStoreService:
    """
    Service for interacting with a ChromaDB vector store.
    Manages adding, searching, and deleting document chunks.
    """

    def __init__(
        self,
        settings: Optional[AppSettings] = None,
        embedding_service: Optional[EmbeddingService] = None, # Pass if using external embeddings
    ):
        """
        Initializes the VectorStoreService.

        Args:
            settings: Application settings. If None, loads default settings.
            embedding_service: An instance of EmbeddingService. Required if not using Chroma's
                               default sentence transformer. If provided, its model name will
                               be used for Chroma's ef.
        """
        if settings is None:
            settings = get_settings()

        self.persist_directory: str = settings.VECTOR_STORE_PATH
        self.collection_name: str = settings.CHROMA_COLLECTION_NAME
        self._embedding_service = embedding_service # Store for potential use (e.g. dimension)

        # Determine embedding function for ChromaDB
        # If an embedding_service is provided and has a model_name, use it.
        # Otherwise, ChromaDB will use its default if ef is not specified or is SentenceTransformerEmbeddingFunction.
        chroma_ef = None
        if self._embedding_service and self._embedding_service.model_name:
            logger.info(f"Using embedding model '{self._embedding_service.model_name}' for ChromaDB.")
            chroma_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=self._embedding_service.model_name,
                device=self._embedding_service.device or "cpu" # Ensure device is passed
            )
        else:
            # Fallback to default SentenceTransformer if no specific model from EmbeddingService
            # This is Chroma's default behavior if ef is not set, but explicit can be clearer.
            default_ef_model = settings.EMBEDDING_MODEL_NAME # Use the one from settings
            logger.info(f"Using default embedding model '{default_ef_model}' for ChromaDB.")
            chroma_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=default_ef_model,
                device=settings.EMBEDDING_MODEL_DEVICE or "cpu"
            )


        try:
            logger.info(
                f"Initializing ChromaDB client with persistence directory: {self.persist_directory}"
            )
            self.client = chromadb.PersistentClient(path=self.persist_directory)

            # Get or create the collection
            # We pass the embedding function here. If it's None, Chroma uses its default.
            logger.info(f"Getting or creating ChromaDB collection: {self.collection_name}")
            self.collection: ChromaCollection = self.client.get_or_create_collection(
                name=self.collection_name,
                embedding_function=chroma_ef, # Can be None for Chroma's default
                # metadata={"hnsw:space": "cosine"} # Example: configure distance metric
            )
            logger.info(
                f"Vector store initialized. Collection '{self.collection_name}' ready. "
                f"Items in collection: {self.collection.count()}"
            )

        except Exception as e:
            logger.error(
                f"Failed to initialize ChromaDB client or collection '{self.collection_name}': {e}",
                exc_info=True,
            )
            raise VectorStoreError(
                "ChromaDB client/collection initialization failed."
            ) from e

    def add_chunks(self, chunks: List[DocumentChunk]) -> None:
        """
        Adds document chunks to the vector store.
        Embeddings are generated by ChromaDB's configured embedding function.

        Args:
            chunks: A list of DocumentChunk objects. Embeddings should NOT be pre-populated
                    if ChromaDB is to generate them.

        Raises:
            VectorStoreError: If adding chunks fails.
        """
        if not chunks:
            logger.info("No chunks provided to add to the vector store.")
            return

        ids: List[str] = []
        documents_content: List[str] = []
        metadatas: List[Dict[str, Any]] = []

        for chunk in chunks:
            if not chunk.content:
                logger.warning(f"Chunk {chunk.id} has no content, skipping.")
                continue
            ids.append(chunk.id)
            documents_content.append(chunk.content)
            # ChromaDB metadata values must be str, int, float, or bool
            # Pydantic models in metadata need to be converted to dicts of compatible types
            meta_dict = chunk.metadata.model_dump(exclude_none=True)
            # Ensure all values in meta_dict are Chroma-compatible
            compatible_meta = {
                k: (str(v) if not isinstance(v, (str, int, float, bool)) else v)
                for k, v in meta_dict.items()
            }
            metadatas.append(compatible_meta)


        if not ids:
            logger.info("No valid chunks with content to add after filtering.")
            return

        try:
            logger.debug(f"Adding {len(ids)} chunks to collection '{self.collection_name}'.")
            # ChromaDB generates embeddings internally if not provided
            self.collection.add(
                ids=ids,
                documents=documents_content, # Pass raw text content for Chroma to embed
                metadatas=metadatas,
            )
            logger.info(f"Successfully added/updated {len(ids)} chunks to '{self.collection_name}'.")
        except Exception as e:
            logger.error(
                f"Failed to add chunks to collection '{self.collection_name}': {e}",
                exc_info=True,
            )
            raise VectorStoreError("Failed to add chunks to vector store.") from e

    def search_similar(
        self, 
        query_embedding: List[float], 
        top_k: int = 3, 
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[RetrievedChunk]:
        """Search for chunks similar to the query embedding."""
        try:
            logger.info(f"Searching for {top_k} similar chunks in collection '{self.collection_name}'.")
            
            results = self.collection.query(
                query_embeddings=[query_embedding], # Must be a list of embeddings
                n_results=top_k,
                where=filter_metadata, # Optional metadata filter
                include=["metadatas", "documents", "distances"], # Request distances (similarity scores)
            )
            
            # ChromaDB returns distances. We need to handle different distance metrics properly.
            # The relationship between distance and similarity depends on the distance metric:
            # - For cosine: similarity = 1 - distance (but distance can be > 1 for very dissimilar items)
            # - For L2/euclidean: smaller distance = higher similarity
            # - For inner product: higher value = higher similarity
            
            retrieved_chunks: List[RetrievedChunk] = []
            
            if results["ids"] and len(results["ids"][0]) > 0:
                for i in range(len(results["ids"][0])):
                    chunk_id = results["ids"][0][i]
                    content = results["documents"][0][i] if results["documents"] else ""
                    metadata_dict = results["metadatas"][0][i] if results["metadatas"] else {}
                    distance = results["distances"][0][i] if results["distances"] else float('inf')
                    
                    # Convert distance to similarity score (0-1 range)
                    # For cosine distance, similarity = 1 - distance, but clamp to [0, 1]
                    # This handles cases where distance > 1 (very dissimilar items)
                    if distance is not None:
                        similarity_score = max(0.0, min(1.0, 1.0 - distance))
                    else:
                        similarity_score = 0.0
                    
                    retrieved_chunk = RetrievedChunk(
                        id=chunk_id,
                        content=content,
                        metadata=metadata_dict,
                        score=similarity_score
                    )
                    retrieved_chunks.append(retrieved_chunk)
            
            logger.info(f"Found {len(retrieved_chunks)} similar chunks.")
            return retrieved_chunks
            
        except Exception as e:
            logger.error(f"Failed to search in collection '{self.collection_name}': {e}", exc_info=True)
            raise VectorStoreError("Failed to search for similar chunks.") from e

    def delete_chunks(self, chunk_ids: Optional[List[str]] = None, filter_metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Deletes chunks from the vector store by IDs or metadata filter.

        Args:
            chunk_ids: A list of chunk IDs to delete.
            filter_metadata: A dictionary to filter chunks for deletion.

        Raises:
            VectorStoreError: If deletion fails.
        """
        if not chunk_ids and not filter_metadata:
            logger.warning("Attempted to delete chunks without specifying IDs or a filter.")
            return

        try:
            if chunk_ids:
                logger.info(f"Deleting {len(chunk_ids)} chunks by ID from '{self.collection_name}'.")
                self.collection.delete(ids=chunk_ids)
            elif filter_metadata: # `where` in ChromaDB
                logger.info(f"Deleting chunks by metadata filter from '{self.collection_name}': {filter_metadata}")
                self.collection.delete(where=filter_metadata)
            logger.info("Chunks deleted successfully.")
        except Exception as e:
            logger.error(
                f"Failed to delete chunks from '{self.collection_name}': {e}",
                exc_info=True,
            )
            raise VectorStoreError("Failed to delete chunks from vector store.") from e

    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Retrieves statistics about the collection.

        Returns:
            A dictionary containing collection statistics (e.g., item count).
        """
        try:
            count = self.collection.count()
            # name = self.collection.name # Already known
            # metadata = self.collection.metadata # If any was set
            stats = {"item_count": count, "collection_name": self.collection_name}
            logger.debug(f"Collection '{self.collection_name}' stats: {stats}")
            return stats
        except Exception as e:
            logger.error(
                f"Failed to get stats for collection '{self.collection_name}': {e}",
                exc_info=True,
            )
            raise VectorStoreError("Failed to retrieve collection statistics.") from e

    def clear_collection(self) -> None:
        """
        Deletes all items from the collection.
        This is a destructive operation.
        """
        try:
            current_count = self.collection.count()
            if current_count == 0:
                logger.info(f"Collection '{self.collection_name}' is already empty. No items to clear.")
                return

            logger.warning(
                f"Clearing all {current_count} items from collection '{self.collection_name}'. "
                "This operation is destructive."
            )
            # To delete all items, one way is to get all IDs and delete them.
            # Or, delete and recreate the collection (more robust for full clear).
            self.client.delete_collection(name=self.collection_name)
            logger.info(f"Collection '{self.collection_name}' deleted.")
            # Recreate it with the same embedding function
            chroma_ef = None
            if self._embedding_service and self._embedding_service.model_name:
                 chroma_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name=self._embedding_service.model_name,
                    device=self._embedding_service.device or "cpu"
                )
            else:
                settings = get_settings()
                default_ef_model = settings.EMBEDDING_MODEL_NAME
                chroma_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name=default_ef_model,
                    device=settings.EMBEDDING_MODEL_DEVICE or "cpu"
                )

            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                embedding_function=chroma_ef
            )
            logger.info(f"Collection '{self.collection_name}' recreated and is now empty.")

        except Exception as e:
            logger.error(
                f"Failed to clear collection '{self.collection_name}': {e}",
                exc_info=True,
            )
            raise VectorStoreError(f"Failed to clear collection '{self.collection_name}'.") from e


# Example Usage:
if __name__ == "__main__":
    from rag_system.config.logging_config import setup_logging
    setup_logging("DEBUG")

    # Create dummy settings for example
    class MockSettings(AppSettings):
        VECTOR_STORE_PATH: str = "./data/test_chroma_store" # Use a test-specific path
        CHROMA_COLLECTION_NAME: str = "test_collection"
        EMBEDDING_MODEL_NAME: str = "sentence-transformers/all-MiniLM-L6-v2" # Ensure this is consistent

    test_settings = MockSettings()
    Path(test_settings.VECTOR_STORE_PATH).mkdir(parents=True, exist_ok=True)

    try:
        # Initialize services
        embed_service = EmbeddingService(settings=test_settings)
        vector_store = VectorStoreService(settings=test_settings, embedding_service=embed_service)

        # Clear collection for a fresh start in example
        logger.info(f"Attempting to clear collection '{vector_store.collection_name}' for test run.")
        vector_store.clear_collection()
        logger.info(f"Initial collection stats: {vector_store.get_collection_stats()}")


        # Create sample chunks
        sample_chunks_data = [
            ("id1", "This is the first document about apples.", {"source": "docA", "type": "fruit"}),
            ("id2", "The second document discusses bananas.", {"source": "docB", "type": "fruit"}),
            ("id3", "Oranges are also a popular citrus fruit.", {"source": "docC", "type": "fruit"}),
            ("id4", "A document about cars and engines.", {"source": "docD", "type": "vehicle"}),
        ]
        chunks_to_add: List[DocumentChunk] = []
        for id_val, content_val, meta_val in sample_chunks_data:
            doc_meta = DocumentMetadata(
                source_id=meta_val["source"],
                filename=f"{meta_val['source']}.txt",
                custom_fields=meta_val # Store additional fields
            )
            chunks_to_add.append(
                DocumentChunk(id=id_val, document_id=meta_val["source"], content=content_val, metadata=doc_meta)
            )

        # Add chunks
        vector_store.add_chunks(chunks_to_add)
        logger.info(f"Collection stats after adding chunks: {vector_store.get_collection_stats()}")

        # Search for similar chunks
        query_text = "Tell me about fruits"
        query_embedding = embed_service.encode_query(query_text)

        if query_embedding:
            similar_chunks = vector_store.search_similar(query_embedding, top_k=2)
            logger.info(f"Found {len(similar_chunks)} similar chunks for query: '{query_text}'")
            for i, r_chunk in enumerate(similar_chunks):
                logger.info(f"Result {i+1}: ID={r_chunk.id}, Score={r_chunk.score:.4f}, Content='{r_chunk.content[:50]}...'")
                logger.info(f"   Metadata: {r_chunk.metadata}")

            # Search with metadata filter
            filtered_chunks = vector_store.search_similar(
                query_embedding, top_k=2, filter_metadata={"type": "vehicle"} # Chroma syntax for where
            )
            logger.info(f"Found {len(filtered_chunks)} similar chunks with filter {{'type': 'vehicle'}}:")
            for i, r_chunk in enumerate(filtered_chunks):
                logger.info(f"Filtered Result {i+1}: ID={r_chunk.id}, Score={r_chunk.score:.4f}, Content='{r_chunk.content[:50]}...'")
        else:
            logger.error("Could not generate query embedding for search.")


        # Delete a chunk
        vector_store.delete_chunks(chunk_ids=["id1"])
        logger.info(f"Collection stats after deleting 'id1': {vector_store.get_collection_stats()}")

        # Delete by metadata
        vector_store.delete_chunks(filter_metadata={"type": "fruit"})
        logger.info(f"Collection stats after deleting fruit type: {vector_store.get_collection_stats()}")


    except (VectorStoreError, EmbeddingError) as e:
        logger.error(f"Vector store or embedding service error: {e}")
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}", exc_info=True)
    finally:
        # Clean up test directory (optional)
        # import shutil
        # if Path(test_settings.VECTOR_STORE_PATH).exists():
        #     shutil.rmtree(test_settings.VECTOR_STORE_PATH)
        #     logger.info(f"Cleaned up test vector store: {test_settings.VECTOR_STORE_PATH}")
        pass
